{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3caa7215-933c-41d2-83dd-a26af3df42e9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Concept\n",
    "- When Spark is reading a file, for example, a parquet, it split the data into partitions to use parallel processing.\n",
    "- So, maxPartitionByteSize defines the **maximum size of each one of this partitions**. It means that a higher value will have less partitions and so, less jobs. A smaller value will have more partitions and so, more jobs.\n",
    "- By default, this value is defined as **128mb**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7f45c604-222b-4bbb-8cfd-2b64ccbca86b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "sc.setJobDescription(\"Step A-1: Basic initialization\")\n",
    "spark.conf.set(\"spark.databricks.io.cache.enabled\", \"false\")               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b84e33f1-80c0-4fc0-87e4-988acbafbd2e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<table>\n",
       "  <tr><td>Max Partition Bytes:</td><td><b>128.0</b> MB</td></tr>\n",
       "  <tr><td>Open Cost In Bytes: </td><td><b>4.0</b> MB</td></tr>\n",
       "</table>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "defaultMaxPartitionBytes = int(spark.conf.get(\"spark.sql.files.maxPartitionBytes\").replace(\"b\",\"\"))\n",
    "openCostInBytes = int(spark.conf.get(\"spark.sql.files.openCostInBytes\").replace(\"b\",\"\"))\n",
    "displayHTML(f\"\"\"<table>\n",
    "  <tr><td>Max Partition Bytes:</td><td><b>{defaultMaxPartitionBytes/1024/1024}</b> MB</td></tr>\n",
    "  <tr><td>Open Cost In Bytes: </td><td><b>{openCostInBytes/1024/1024}</b> MB</td></tr>\n",
    "</table>\"\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f36ccb0d-bd0c-4a29-bb75-bfdf903780e3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "1. Set two spark properties\n",
    "\t1. `spark.sql.files.maxPartitionBytes` → default **128 MB**.\n",
    "\t2. `spark.sql.files.openCostInBytes` → default cost Spark adds for each file (helps balance tiny files).\n",
    "\t\t1. It’s the **estimated cost (in bytes)** Spark assumes just for opening a file, even before reading its data.\n",
    "2. Adjusting it for local development, I'm reading the data from Responsys.\n",
    "3. Initially, I set the table schema and create the prediction for num_of_partitions, considering `Padded_Bytes / Target_Size`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "34f9377b-edb9-420c-b084-fb89f2e533f0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "sc.setJobDescription(\"Step A-2: Utility Function\")\n",
    "def predict_num_partitions(files):\n",
    "    import math\n",
    "    open_cost = int(spark.conf.get(\"spark.sql.files.openCostInBytes\").replace(\"b\", \"\"))\n",
    "    max_partition_bytes = int(spark.conf.get(\"spark.sql.files.maxPartitionBytes\").replace(\"b\", \"\"))\n",
    "\n",
    "    actual_bytes = sum(f.size for f in files)\n",
    "    padded_bytes = actual_bytes + (len(files) * open_cost)\n",
    "\n",
    "    bytes_per_core = padded_bytes // sc.defaultParallelism\n",
    "    max_of_cost_bpc = max(open_cost, bytes_per_core)\n",
    "    target_size = min(max_partition_bytes, max_of_cost_bpc)\n",
    "    partitions = padded_bytes / target_size\n",
    "\n",
    "    def row(label, value, extra=\"\"):\n",
    "        return f'<tr><td>{label}:</td><td style=\"text-align:right; font-weight:bold\">{value:,}</td><td style=\"padding-left:1em\">{extra}</td></tr>'\n",
    "\n",
    "    html = \"<table>\" + \\\n",
    "        row(\"File Count\", len(files)) + \\\n",
    "        row(\"Actual Bytes\", actual_bytes) + \\\n",
    "        row(\"Padded Bytes\", padded_bytes, \"Actual_Bytes + (File_Count * Open_Cost)\") + \\\n",
    "        row(\"Average Size\", padded_bytes // len(files)) + \\\n",
    "        '<tr><td colspan=\"2\" style=\"border-top:1px solid black\">&nbsp;</td></tr>' + \\\n",
    "        row(\"Open Cost\", open_cost, \"spark.sql.files.openCostInBytes\") + \\\n",
    "        row(\"Bytes-Per-Core\", bytes_per_core) + \\\n",
    "        row(\"Max Cost\", max_of_cost_bpc, \"(max of Open_Cost & Bytes-Per-Core)\") + \\\n",
    "        '<tr><td colspan=\"2\" style=\"border-top:1px solid black\">&nbsp;</td></tr>' + \\\n",
    "        row(\"Max Partition Bytes\", max_partition_bytes, \"spark.sql.files.maxPartitionBytes\") + \\\n",
    "        row(\"Target Size\", target_size, \"(min of Max_Cost & Max_Partition_Bytes)\") + \\\n",
    "        '<tr><td colspan=\"2\" style=\"border-top:1px solid black\">&nbsp;</td></tr>' + \\\n",
    "        row(\"Number of Partitions\", math.ceil(partitions), f\"({partitions} from Padded_Bytes / Target_Size)\") + \\\n",
    "        \"</table>\"\n",
    "    displayHTML(html)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6c4c3807-0e7a-49f9-8768-39e9aade147d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import window, col\n",
    "from pyspark.sql.types import StructType, StructField, DecimalType, StringType\n",
    "\n",
    "\n",
    "trxPath = \"s3a://tks-dados-responsys/EXPORT_FILES/files/STATUS_OPT/\"\n",
    "trxFiles = [f for f in dbutils.fs.ls(trxPath) if f.name.endswith(\".parquet\")]\n",
    "trxSchema = StructType([\n",
    "    StructField(\"_SDC_SOURCE_LINENO\", DecimalType(38, 0)),\n",
    "    StructField(\"EMAIL_ADDRESS_\", StringType()),\n",
    "    StructField(\"EMAIL_PERMISSION_STATUS_\", StringType()),\n",
    "    StructField(\"_SDC_SOURCE_FILE\", StringType()),\n",
    "    StructField(\"_SDC_SEQUENCE\", DecimalType(38, 0)),\n",
    "    StructField(\"_SDC_RECEIVED_AT\", StringType()),\n",
    "    StructField(\"_SDC_BATCHED_AT\", StringType()),\n",
    "    StructField(\"_SDC_TABLE_VERSION\", DecimalType(38, 0)),\n",
    "])\n",
    "\n",
    "sc.setJobDescription(\"Step C: Read at 1x\")\n",
    "maxPartitionBytesConf = 1 * int(defaultMaxPartitionBytes)\n",
    "spark.conf.set(\"spark.sql.files.maxPartitionBytes\", f\"{maxPartitionBytesConf}b\")\n",
    "spark.read.schema(trxSchema).parquet(trxPath).write.format(\"noop\").mode(\"overwrite\").save()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e4a5223e-b6b4-45b2-b82f-929b80eb35e6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<table><tr><td>File Count:</td><td style=\"text-align:right; font-weight:bold\">1,466</td><td style=\"padding-left:1em\"></td></tr><tr><td>Actual Bytes:</td><td style=\"text-align:right; font-weight:bold\">654,624,309</td><td style=\"padding-left:1em\"></td></tr><tr><td>Padded Bytes:</td><td style=\"text-align:right; font-weight:bold\">6,803,473,973</td><td style=\"padding-left:1em\">Actual_Bytes + (File_Count * Open_Cost)</td></tr><tr><td>Average Size:</td><td style=\"text-align:right; font-weight:bold\">4,640,841</td><td style=\"padding-left:1em\"></td></tr><tr><td colspan=\"2\" style=\"border-top:1px solid black\">&nbsp;</td></tr><tr><td>Open Cost:</td><td style=\"text-align:right; font-weight:bold\">4,194,304</td><td style=\"padding-left:1em\">spark.sql.files.openCostInBytes</td></tr><tr><td>Bytes-Per-Core:</td><td style=\"text-align:right; font-weight:bold\">3,401,736,986</td><td style=\"padding-left:1em\"></td></tr><tr><td>Max Cost:</td><td style=\"text-align:right; font-weight:bold\">3,401,736,986</td><td style=\"padding-left:1em\">(max of Open_Cost & Bytes-Per-Core)</td></tr><tr><td colspan=\"2\" style=\"border-top:1px solid black\">&nbsp;</td></tr><tr><td>Max Partition Bytes:</td><td style=\"text-align:right; font-weight:bold\">134,217,728</td><td style=\"padding-left:1em\">spark.sql.files.maxPartitionBytes</td></tr><tr><td>Target Size:</td><td style=\"text-align:right; font-weight:bold\">134,217,728</td><td style=\"padding-left:1em\">(min of Max_Cost & Max_Partition_Bytes)</td></tr><tr><td colspan=\"2\" style=\"border-top:1px solid black\">&nbsp;</td></tr><tr><td>Number of Partitions:</td><td style=\"text-align:right; font-weight:bold\">51</td><td style=\"padding-left:1em\">(50.68983117491007 from Padded_Bytes / Target_Size)</td></tr></table>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "predict_num_partitions(trxFiles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "329a0a68-dd57-400b-b858-d826c5259e7d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "From the SparkUi we have the following:\n",
    "![](sparkUi_x1.png)\n",
    "\n",
    " **Number of tasks** → **51/51**\n",
    "\t    - This matches the number of partitions Spark created for the read.\n",
    "\t    - Since `maxPartitionBytes` is ~128 MB, Spark split the dataset into 51 chunks (considering file sizes + open cost).\n",
    "\t    - In `predict_num_partitions()` output, we can see 51 too.\n",
    "      \n",
    "**Input size** → **773.9 MiB** total\n",
    "\t    - This is the total amount of data Spark read from the parquet files.\n",
    "\t    - Dividing roughly: 773.9 MiB ÷ 51 ≈ 15 MB per task — this is smaller than 128 MB because:\n",
    "\t        - Many files might be much smaller than 128 MB.\n",
    "\t        - The open cost and file boundaries prevent perfect packing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bfda96e1-90ab-44eb-9fde-54b346fa26c8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<table><tr><td>File Count:</td><td style=\"text-align:right; font-weight:bold\">1,466</td><td style=\"padding-left:1em\"></td></tr><tr><td>Actual Bytes:</td><td style=\"text-align:right; font-weight:bold\">654,624,309</td><td style=\"padding-left:1em\"></td></tr><tr><td>Padded Bytes:</td><td style=\"text-align:right; font-weight:bold\">6,803,473,973</td><td style=\"padding-left:1em\">Actual_Bytes + (File_Count * Open_Cost)</td></tr><tr><td>Average Size:</td><td style=\"text-align:right; font-weight:bold\">4,640,841</td><td style=\"padding-left:1em\"></td></tr><tr><td colspan=\"2\" style=\"border-top:1px solid black\">&nbsp;</td></tr><tr><td>Open Cost:</td><td style=\"text-align:right; font-weight:bold\">4,194,304</td><td style=\"padding-left:1em\">spark.sql.files.openCostInBytes</td></tr><tr><td>Bytes-Per-Core:</td><td style=\"text-align:right; font-weight:bold\">3,401,736,986</td><td style=\"padding-left:1em\"></td></tr><tr><td>Max Cost:</td><td style=\"text-align:right; font-weight:bold\">3,401,736,986</td><td style=\"padding-left:1em\">(max of Open_Cost & Bytes-Per-Core)</td></tr><tr><td colspan=\"2\" style=\"border-top:1px solid black\">&nbsp;</td></tr><tr><td>Max Partition Bytes:</td><td style=\"text-align:right; font-weight:bold\">268,435,456</td><td style=\"padding-left:1em\">spark.sql.files.maxPartitionBytes</td></tr><tr><td>Target Size:</td><td style=\"text-align:right; font-weight:bold\">268,435,456</td><td style=\"padding-left:1em\">(min of Max_Cost & Max_Partition_Bytes)</td></tr><tr><td colspan=\"2\" style=\"border-top:1px solid black\">&nbsp;</td></tr><tr><td>Number of Partitions:</td><td style=\"text-align:right; font-weight:bold\">26</td><td style=\"padding-left:1em\">(25.344915587455034 from Padded_Bytes / Target_Size)</td></tr></table>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sc.setJobDescription(\"Step D: Read at 2x\")\n",
    "maxPartitionBytesConf = 2 * int(defaultMaxPartitionBytes)\n",
    "spark.conf.set(\"spark.sql.files.maxPartitionBytes\", f\"{maxPartitionBytesConf}b\")\n",
    "predict_num_partitions(trxFiles)\n",
    "spark.read.schema(trxSchema).parquet(trxPath).write.format(\"noop\").mode(\"overwrite\").save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7625db3b-22d8-4d93-8787-69b9edf72766",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Tasks → 26/26**\n",
    "- We doubled maxPartitionBytes from ~128 MB to ~256 MB.\n",
    "- That let Spark put more data in each partition, so it needed about half as many partitions as before (51 → 26).\n",
    "- This matches exactly what we expected from the theory.\n",
    "\n",
    "**Duration → 12 seconds**\n",
    "- Huge improvement from 2.3 minutes in Step C!\n",
    "- Less scheduling overhead (fewer tasks to start and finish).\n",
    "- Each task processes more data at once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b55e38cc-3bf3-4b4d-85d4-54138e1c9ec8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<table><tr><td>File Count:</td><td style=\"text-align:right; font-weight:bold\">1,466</td><td style=\"padding-left:1em\"></td></tr><tr><td>Actual Bytes:</td><td style=\"text-align:right; font-weight:bold\">654,624,309</td><td style=\"padding-left:1em\"></td></tr><tr><td>Padded Bytes:</td><td style=\"text-align:right; font-weight:bold\">6,803,473,973</td><td style=\"padding-left:1em\">Actual_Bytes + (File_Count * Open_Cost)</td></tr><tr><td>Average Size:</td><td style=\"text-align:right; font-weight:bold\">4,640,841</td><td style=\"padding-left:1em\"></td></tr><tr><td colspan=\"2\" style=\"border-top:1px solid black\">&nbsp;</td></tr><tr><td>Open Cost:</td><td style=\"text-align:right; font-weight:bold\">4,194,304</td><td style=\"padding-left:1em\">spark.sql.files.openCostInBytes</td></tr><tr><td>Bytes-Per-Core:</td><td style=\"text-align:right; font-weight:bold\">3,401,736,986</td><td style=\"padding-left:1em\"></td></tr><tr><td>Max Cost:</td><td style=\"text-align:right; font-weight:bold\">3,401,736,986</td><td style=\"padding-left:1em\">(max of Open_Cost & Bytes-Per-Core)</td></tr><tr><td colspan=\"2\" style=\"border-top:1px solid black\">&nbsp;</td></tr><tr><td>Max Partition Bytes:</td><td style=\"text-align:right; font-weight:bold\">536,870,912</td><td style=\"padding-left:1em\">spark.sql.files.maxPartitionBytes</td></tr><tr><td>Target Size:</td><td style=\"text-align:right; font-weight:bold\">536,870,912</td><td style=\"padding-left:1em\">(min of Max_Cost & Max_Partition_Bytes)</td></tr><tr><td colspan=\"2\" style=\"border-top:1px solid black\">&nbsp;</td></tr><tr><td>Number of Partitions:</td><td style=\"text-align:right; font-weight:bold\">13</td><td style=\"padding-left:1em\">(12.672457793727517 from Padded_Bytes / Target_Size)</td></tr></table>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sc.setJobDescription(\"Step E: Read at 4x\")\n",
    "maxPartitionBytesConf = 4 * int(defaultMaxPartitionBytes)\n",
    "spark.conf.set(\"spark.sql.files.maxPartitionBytes\", f\"{maxPartitionBytesConf}b\")\n",
    "predict_num_partitions(trxFiles)\n",
    "spark.read.schema(trxSchema).parquet(trxPath).write.format(\"noop\").mode(\"overwrite\").save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "01affeae-b7f8-4ff6-a831-67bd4746d474",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<table><tr><td>File Count:</td><td style=\"text-align:right; font-weight:bold\">1,466</td><td style=\"padding-left:1em\"></td></tr><tr><td>Actual Bytes:</td><td style=\"text-align:right; font-weight:bold\">654,624,309</td><td style=\"padding-left:1em\"></td></tr><tr><td>Padded Bytes:</td><td style=\"text-align:right; font-weight:bold\">6,803,473,973</td><td style=\"padding-left:1em\">Actual_Bytes + (File_Count * Open_Cost)</td></tr><tr><td>Average Size:</td><td style=\"text-align:right; font-weight:bold\">4,640,841</td><td style=\"padding-left:1em\"></td></tr><tr><td colspan=\"2\" style=\"border-top:1px solid black\">&nbsp;</td></tr><tr><td>Open Cost:</td><td style=\"text-align:right; font-weight:bold\">4,194,304</td><td style=\"padding-left:1em\">spark.sql.files.openCostInBytes</td></tr><tr><td>Bytes-Per-Core:</td><td style=\"text-align:right; font-weight:bold\">3,401,736,986</td><td style=\"padding-left:1em\"></td></tr><tr><td>Max Cost:</td><td style=\"text-align:right; font-weight:bold\">3,401,736,986</td><td style=\"padding-left:1em\">(max of Open_Cost & Bytes-Per-Core)</td></tr><tr><td colspan=\"2\" style=\"border-top:1px solid black\">&nbsp;</td></tr><tr><td>Max Partition Bytes:</td><td style=\"text-align:right; font-weight:bold\">1,073,741,824</td><td style=\"padding-left:1em\">spark.sql.files.maxPartitionBytes</td></tr><tr><td>Target Size:</td><td style=\"text-align:right; font-weight:bold\">1,073,741,824</td><td style=\"padding-left:1em\">(min of Max_Cost & Max_Partition_Bytes)</td></tr><tr><td colspan=\"2\" style=\"border-top:1px solid black\">&nbsp;</td></tr><tr><td>Number of Partitions:</td><td style=\"text-align:right; font-weight:bold\">7</td><td style=\"padding-left:1em\">(6.336228896863759 from Padded_Bytes / Target_Size)</td></tr></table>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sc.setJobDescription(\"Step F: Read at 8x\")\n",
    "maxPartitionBytesConf = 8 * int(defaultMaxPartitionBytes)\n",
    "spark.conf.set(\"spark.sql.files.maxPartitionBytes\", f\"{maxPartitionBytesConf}b\")\n",
    "predict_num_partitions(trxFiles)\n",
    "spark.read.schema(trxSchema).parquet(trxPath).write.format(\"noop\").mode(\"overwrite\").save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b42d9c75-da7b-4e98-95a9-4e0439c592d2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<table><tr><td>File Count:</td><td style=\"text-align:right; font-weight:bold\">1,466</td><td style=\"padding-left:1em\"></td></tr><tr><td>Actual Bytes:</td><td style=\"text-align:right; font-weight:bold\">654,624,309</td><td style=\"padding-left:1em\"></td></tr><tr><td>Padded Bytes:</td><td style=\"text-align:right; font-weight:bold\">6,803,473,973</td><td style=\"padding-left:1em\">Actual_Bytes + (File_Count * Open_Cost)</td></tr><tr><td>Average Size:</td><td style=\"text-align:right; font-weight:bold\">4,640,841</td><td style=\"padding-left:1em\"></td></tr><tr><td colspan=\"2\" style=\"border-top:1px solid black\">&nbsp;</td></tr><tr><td>Open Cost:</td><td style=\"text-align:right; font-weight:bold\">4,194,304</td><td style=\"padding-left:1em\">spark.sql.files.openCostInBytes</td></tr><tr><td>Bytes-Per-Core:</td><td style=\"text-align:right; font-weight:bold\">3,401,736,986</td><td style=\"padding-left:1em\"></td></tr><tr><td>Max Cost:</td><td style=\"text-align:right; font-weight:bold\">3,401,736,986</td><td style=\"padding-left:1em\">(max of Open_Cost & Bytes-Per-Core)</td></tr><tr><td colspan=\"2\" style=\"border-top:1px solid black\">&nbsp;</td></tr><tr><td>Max Partition Bytes:</td><td style=\"text-align:right; font-weight:bold\">2,147,483,648</td><td style=\"padding-left:1em\">spark.sql.files.maxPartitionBytes</td></tr><tr><td>Target Size:</td><td style=\"text-align:right; font-weight:bold\">2,147,483,648</td><td style=\"padding-left:1em\">(min of Max_Cost & Max_Partition_Bytes)</td></tr><tr><td colspan=\"2\" style=\"border-top:1px solid black\">&nbsp;</td></tr><tr><td>Number of Partitions:</td><td style=\"text-align:right; font-weight:bold\">4</td><td style=\"padding-left:1em\">(3.1681144484318793 from Padded_Bytes / Target_Size)</td></tr></table>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sc.setJobDescription(\"Step G: Read at 16x\")\n",
    "maxPartitionBytesConf = 16 * int(defaultMaxPartitionBytes)\n",
    "spark.conf.set(\"spark.sql.files.maxPartitionBytes\", f\"{maxPartitionBytesConf}b\")\n",
    "predict_num_partitions(trxFiles)\n",
    "spark.read.schema(trxSchema).parquet(trxPath).write.format(\"noop\").mode(\"overwrite\").save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3e200bfe-585c-425b-9ae7-87e7ad40f90d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<table><tr><td>File Count:</td><td style=\"text-align:right; font-weight:bold\">1,466</td><td style=\"padding-left:1em\"></td></tr><tr><td>Actual Bytes:</td><td style=\"text-align:right; font-weight:bold\">654,624,309</td><td style=\"padding-left:1em\"></td></tr><tr><td>Padded Bytes:</td><td style=\"text-align:right; font-weight:bold\">6,803,473,973</td><td style=\"padding-left:1em\">Actual_Bytes + (File_Count * Open_Cost)</td></tr><tr><td>Average Size:</td><td style=\"text-align:right; font-weight:bold\">4,640,841</td><td style=\"padding-left:1em\"></td></tr><tr><td colspan=\"2\" style=\"border-top:1px solid black\">&nbsp;</td></tr><tr><td>Open Cost:</td><td style=\"text-align:right; font-weight:bold\">4,194,304</td><td style=\"padding-left:1em\">spark.sql.files.openCostInBytes</td></tr><tr><td>Bytes-Per-Core:</td><td style=\"text-align:right; font-weight:bold\">3,401,736,986</td><td style=\"padding-left:1em\"></td></tr><tr><td>Max Cost:</td><td style=\"text-align:right; font-weight:bold\">3,401,736,986</td><td style=\"padding-left:1em\">(max of Open_Cost & Bytes-Per-Core)</td></tr><tr><td colspan=\"2\" style=\"border-top:1px solid black\">&nbsp;</td></tr><tr><td>Max Partition Bytes:</td><td style=\"text-align:right; font-weight:bold\">4,294,967,296</td><td style=\"padding-left:1em\">spark.sql.files.maxPartitionBytes</td></tr><tr><td>Target Size:</td><td style=\"text-align:right; font-weight:bold\">3,401,736,986</td><td style=\"padding-left:1em\">(min of Max_Cost & Max_Partition_Bytes)</td></tr><tr><td colspan=\"2\" style=\"border-top:1px solid black\">&nbsp;</td></tr><tr><td>Number of Partitions:</td><td style=\"text-align:right; font-weight:bold\">3</td><td style=\"padding-left:1em\">(2.0000000002939675 from Padded_Bytes / Target_Size)</td></tr></table>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sc.setJobDescription(\"Step H: Read at 32x\")\n",
    "maxPartitionBytesConf = 32 * int(defaultMaxPartitionBytes)\n",
    "spark.conf.set(\"spark.sql.files.maxPartitionBytes\", f\"{maxPartitionBytesConf}b\")\n",
    "predict_num_partitions(trxFiles)\n",
    "spark.read.schema(trxSchema).parquet(trxPath).write.format(\"noop\").mode(\"overwrite\").save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ee39e863-5062-4fcb-82d6-96c4243cd9f9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**We can notice that increasing the maxSize is not always worth it. The first 51 → 26 worked really well, but the next one didn't change anything significantly**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f874a47d-833e-40fd-a1c7-2b2a7b883b24",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "- In theory, the next step would be check how it would perform with files around 128 MB, but I didn't find a bucket with this configuration\n",
    "**- Number of tasks ≈ number of files in the dataset.**\n",
    "\n",
    "- Input per task: roughly the size of a single file.\n",
    "- Increasing maxPartitionBytes won't merge these into bigger partitions because Spark doesn't split files across partitions unless they're big enough.\n",
    "\n",
    "Basically, **if our files are already large, changing maxPartitionBytes won't reduce the number of partitions — the file size itself becomes the limit.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ac516492-5634-4b3f-9eb6-1bad7a3a98f6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def auto_tune_max_partition_bytes(format, path, schema, max_steps, starting_bytes=134217728):\n",
    "    from pyspark.sql import SparkSession\n",
    "    spark = SparkSession.builder.getOrCreate()\n",
    "    sc = spark.sparkContext\n",
    "\n",
    "    sc.setJobDescription(\"Step L-1: Autotune maxPartitionBytes Function\")\n",
    "\n",
    "    cores = sc.defaultParallelism\n",
    "    max_partition_bytes = starting_bytes\n",
    "    original_value = spark.conf.get(\"spark.sql.files.maxPartitionBytes\")\n",
    "\n",
    "    for step in range(max_steps + 1):\n",
    "        max_partition_bytes = starting_bytes + (step * 1024 * 1024)\n",
    "        max_partition_mb = max_partition_bytes // (1024 * 1024)\n",
    "\n",
    "        spark.conf.set(\"spark.sql.files.maxPartitionBytes\", f\"{max_partition_bytes}b\")\n",
    "        df = spark.read.format(format).schema(schema).load(path)\n",
    "        partitions = df.rdd.getNumPartitions\n",
    "\n",
    "        print(f\"{max_partition_mb:,} MB with {partitions:,} partitions, \"\n",
    "              f\"iterations: {partitions / cores:.2f}\")\n",
    "\n",
    "        if partitions % cores == 0:\n",
    "            print(\"*** Found it! ***\")\n",
    "            print(f\"{max_partition_mb:,} MB with {partitions:,} partitions, \"\n",
    "                  f\"iterations: {partitions / cores:.2f}\")\n",
    "            return max_partition_bytes\n",
    "    spark.conf.set(\"spark.sql.files.maxPartitionBytes\", original_value)\n",
    "    raise ValueError(\"An appropriate maxPartitionBytes was not found\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8bfecbb9-539c-4551-8895-e5de0a4caffa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# auto_tune_max_partition_bytes\n",
    "\n",
    "## Purpose\n",
    "\n",
    "The `auto_tune_max_partition_bytes` function is designed to automatically determine an optimal value for the Spark configuration `spark.sql.files.maxPartitionBytes`. This configuration controls the maximum number of bytes to be processed per partition when reading files in Spark.\n",
    "\n",
    "By finding a value that results in a number of partitions **evenly divisible by the number of available cores**, this function aims to achieve better **parallelism and performance** in distributed data processing.\n",
    "\n",
    "---\n",
    "\n",
    "## Parameters\n",
    "\n",
    "| Parameter             | Type    | Description |\n",
    "|-----------------------|---------|-------------|\n",
    "| `format`              | `str`   | The file format (e.g. `\"parquet\"`, `\"csv\"`). |\n",
    "| `path`                | `str`   | Path to the input data files (e.g. S3, DBFS). |\n",
    "| `schema`              | `StructType` | Spark schema to apply when reading the data. |\n",
    "| `max_steps`           | `int`   | The maximum number of tuning attempts. |\n",
    "| `starting_bytes`      | `int`   | Initial value for `maxPartitionBytes` in bytes (default: 134,217,728 bytes = 128 MB). |\n",
    "\n",
    "---\n",
    "\n",
    "## Behavior\n",
    "\n",
    "The function follows these steps:\n",
    "\n",
    "1. Retrieves the number of default parallel tasks (`sc.defaultParallelism`) — typically based on the number of executor cores.\n",
    "2. Starts with a given `starting_bytes` value.\n",
    "3. Iteratively increases the `maxPartitionBytes` value by **1 MB per step** (`step * 1024 * 1024`).\n",
    "4. In each iteration:\n",
    "   - Updates the Spark configuration `spark.sql.files.maxPartitionBytes`.\n",
    "   - Loads the data using the provided schema and format.\n",
    "   - Retrieves the number of partitions created.\n",
    "   - Checks if the number of partitions is divisible evenly by the number of cores.\n",
    "   - If true, it returns the current `maxPartitionBytes` as the optimal value.\n",
    "5. If no optimal value is found after all steps, it restores the original config and raises an error.\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "1. maxPartitionByteSize",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}