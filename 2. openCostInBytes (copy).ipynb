{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "485d3274-e975-491f-9798-0fa6e1fee471",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Now, we kept maxPartitionBytes = 128 MB constant, and only changed openCostInBytes.\n",
    "\n",
    "We used a dataset of 1,467 small parquet files totaling ~654 MB in real size.\n",
    "The goal was to see how changing openCostInBytes affects:\n",
    "\n",
    "- Padded size (real + virtual)\n",
    "- Number of partitions\n",
    "- Spark task count and job runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0fe2707f-6884-4f0e-b599-36ea1c3464d3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sc will be removed in future DBR versions\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<table>\n",
       "  <tr><td>Max Partition Bytes:</td><td><b>128.0</b> MB</td></tr>\n",
       "  <tr><td>Open Cost In Bytes: </td><td><b>4.0</b> MB</td></tr>\n",
       "</table>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sc.setJobDescription(\"Step A-1: Basic initialization\")\n",
    "spark.conf.set(\"spark.databricks.io.cache.enabled\", \"false\")           \n",
    "defaultMaxPartitionBytes = int(spark.conf.get(\"spark.sql.files.maxPartitionBytes\").replace(\"b\",\"\"))\n",
    "openCostInBytes = int(spark.conf.get(\"spark.sql.files.openCostInBytes\").replace(\"b\",\"\"))\n",
    "displayHTML(f\"\"\"<table>\n",
    "  <tr><td>Max Partition Bytes:</td><td><b>{defaultMaxPartitionBytes/1024/1024}</b> MB</td></tr>\n",
    "  <tr><td>Open Cost In Bytes: </td><td><b>{openCostInBytes/1024/1024}</b> MB</td></tr>\n",
    "</table>\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "436d7650-52b7-4905-a39b-be9255bb234d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "sc.setJobDescription(\"Step A-2: Utility Function\")\n",
    "def predict_num_partitions(files):\n",
    "    import math\n",
    "    open_cost = int(spark.conf.get(\"spark.sql.files.openCostInBytes\").replace(\"b\", \"\"))\n",
    "    max_partition_bytes = int(spark.conf.get(\"spark.sql.files.maxPartitionBytes\").replace(\"b\", \"\"))\n",
    "\n",
    "    actual_bytes = sum(f.size for f in files)\n",
    "    padded_bytes = actual_bytes + (len(files) * open_cost)\n",
    "\n",
    "    bytes_per_core = padded_bytes // sc.defaultParallelism\n",
    "    max_of_cost_bpc = max(open_cost, bytes_per_core)\n",
    "    target_size = min(max_partition_bytes, max_of_cost_bpc)\n",
    "    partitions = padded_bytes / target_size\n",
    "\n",
    "    def row(label, value, extra=\"\"):\n",
    "        return f'<tr><td>{label}:</td><td style=\"text-align:right; font-weight:bold\">{value:,}</td><td style=\"padding-left:1em\">{extra}</td></tr>'\n",
    "\n",
    "    html = \"<table>\" + \\\n",
    "        row(\"File Count\", len(files)) + \\\n",
    "        row(\"Actual Bytes\", actual_bytes) + \\\n",
    "        row(\"Padded Bytes\", padded_bytes, \"Actual_Bytes + (File_Count * Open_Cost)\") + \\\n",
    "        row(\"Average Size\", padded_bytes // len(files)) + \\\n",
    "        '<tr><td colspan=\"2\" style=\"border-top:1px solid black\">&nbsp;</td></tr>' + \\\n",
    "        row(\"Open Cost\", open_cost, \"spark.sql.files.openCostInBytes\") + \\\n",
    "        row(\"Bytes-Per-Core\", bytes_per_core) + \\\n",
    "        row(\"Max Cost\", max_of_cost_bpc, \"(max of Open_Cost & Bytes-Per-Core)\") + \\\n",
    "        '<tr><td colspan=\"2\" style=\"border-top:1px solid black\">&nbsp;</td></tr>' + \\\n",
    "        row(\"Max Partition Bytes\", max_partition_bytes, \"spark.sql.files.maxPartitionBytes\") + \\\n",
    "        row(\"Target Size\", target_size, \"(min of Max_Cost & Max_Partition_Bytes)\") + \\\n",
    "        '<tr><td colspan=\"2\" style=\"border-top:1px solid black\">&nbsp;</td></tr>' + \\\n",
    "        row(\"Number of Partitions\", math.ceil(partitions), f\"({partitions} from Padded_Bytes / Target_Size)\") + \\\n",
    "        \"</table>\"\n",
    "    displayHTML(html)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "191f8491-036d-44e2-8b66-87c672351880",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<table><tr><td>File Count:</td><td style=\"text-align:right; font-weight:bold\">1,467</td><td style=\"padding-left:1em\"></td></tr><tr><td>Actual Bytes:</td><td style=\"text-align:right; font-weight:bold\">654,715,376</td><td style=\"padding-left:1em\"></td></tr><tr><td>Padded Bytes:</td><td style=\"text-align:right; font-weight:bold\">6,807,759,344</td><td style=\"padding-left:1em\">Actual_Bytes + (File_Count * Open_Cost)</td></tr><tr><td>Average Size:</td><td style=\"text-align:right; font-weight:bold\">4,640,599</td><td style=\"padding-left:1em\"></td></tr><tr><td colspan=\"2\" style=\"border-top:1px solid black\">&nbsp;</td></tr><tr><td>Open Cost:</td><td style=\"text-align:right; font-weight:bold\">4,194,304</td><td style=\"padding-left:1em\">spark.sql.files.openCostInBytes</td></tr><tr><td>Bytes-Per-Core:</td><td style=\"text-align:right; font-weight:bold\">1,701,939,836</td><td style=\"padding-left:1em\"></td></tr><tr><td>Max Cost:</td><td style=\"text-align:right; font-weight:bold\">1,701,939,836</td><td style=\"padding-left:1em\">(max of Open_Cost & Bytes-Per-Core)</td></tr><tr><td colspan=\"2\" style=\"border-top:1px solid black\">&nbsp;</td></tr><tr><td>Max Partition Bytes:</td><td style=\"text-align:right; font-weight:bold\">134,217,728</td><td style=\"padding-left:1em\">spark.sql.files.maxPartitionBytes</td></tr><tr><td>Target Size:</td><td style=\"text-align:right; font-weight:bold\">134,217,728</td><td style=\"padding-left:1em\">(min of Max_Cost & Max_Partition_Bytes)</td></tr><tr><td colspan=\"2\" style=\"border-top:1px solid black\">&nbsp;</td></tr><tr><td>Number of Partitions:</td><td style=\"text-align:right; font-weight:bold\">51</td><td style=\"padding-left:1em\">(50.72175967693329 from Padded_Bytes / Target_Size)</td></tr></table>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql.functions import window, col\n",
    "from pyspark.sql.types import StructType, StructField, DecimalType, StringType\n",
    "\n",
    "\n",
    "trxPath = \"s3a://tks-dados-responsys/EXPORT_FILES/files/STATUS_OPT/\"\n",
    "trxFiles = [f for f in dbutils.fs.ls(trxPath) if f.name.endswith(\".parquet\")]\n",
    "trxSchema = StructType([\n",
    "    StructField(\"_SDC_SOURCE_LINENO\", DecimalType(38, 0)),\n",
    "    StructField(\"EMAIL_ADDRESS_\", StringType()),\n",
    "    StructField(\"EMAIL_PERMISSION_STATUS_\", StringType()),\n",
    "    StructField(\"_SDC_SOURCE_FILE\", StringType()),\n",
    "    StructField(\"_SDC_SEQUENCE\", DecimalType(38, 0)),\n",
    "    StructField(\"_SDC_RECEIVED_AT\", StringType()),\n",
    "    StructField(\"_SDC_BATCHED_AT\", StringType()),\n",
    "    StructField(\"_SDC_TABLE_VERSION\", DecimalType(38, 0)),\n",
    "])\n",
    "\n",
    "sc.setJobDescription(\"Step C: OCB 4MB\")\n",
    "predict_num_partitions(trxFiles)\n",
    "spark.read.schema(trxSchema).parquet(trxPath).write.format(\"noop\").mode(\"overwrite\").save()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5f8506d4-2af9-47eb-9d77-53188b779bfc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<table><tr><td>File Count:</td><td style=\"text-align:right; font-weight:bold\">1,467</td><td style=\"padding-left:1em\"></td></tr><tr><td>Actual Bytes:</td><td style=\"text-align:right; font-weight:bold\">654,715,376</td><td style=\"padding-left:1em\"></td></tr><tr><td>Padded Bytes:</td><td style=\"text-align:right; font-weight:bold\">1,423,845,872</td><td style=\"padding-left:1em\">Actual_Bytes + (File_Count * Open_Cost)</td></tr><tr><td>Average Size:</td><td style=\"text-align:right; font-weight:bold\">970,583</td><td style=\"padding-left:1em\"></td></tr><tr><td colspan=\"2\" style=\"border-top:1px solid black\">&nbsp;</td></tr><tr><td>Open Cost:</td><td style=\"text-align:right; font-weight:bold\">524,288</td><td style=\"padding-left:1em\">spark.sql.files.openCostInBytes</td></tr><tr><td>Bytes-Per-Core:</td><td style=\"text-align:right; font-weight:bold\">142,384,587</td><td style=\"padding-left:1em\"></td></tr><tr><td>Max Cost:</td><td style=\"text-align:right; font-weight:bold\">142,384,587</td><td style=\"padding-left:1em\">(max of Open_Cost & Bytes-Per-Core)</td></tr><tr><td colspan=\"2\" style=\"border-top:1px solid black\">&nbsp;</td></tr><tr><td>Max Partition Bytes:</td><td style=\"text-align:right; font-weight:bold\">134,217,728</td><td style=\"padding-left:1em\">spark.sql.files.maxPartitionBytes</td></tr><tr><td>Target Size:</td><td style=\"text-align:right; font-weight:bold\">134,217,728</td><td style=\"padding-left:1em\">(min of Max_Cost & Max_Partition_Bytes)</td></tr><tr><td colspan=\"2\" style=\"border-top:1px solid black\">&nbsp;</td></tr><tr><td>Number of Partitions:</td><td style=\"text-align:right; font-weight:bold\">11</td><td style=\"padding-left:1em\">(10.608478426933289 from Padded_Bytes / Target_Size)</td></tr></table>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sc.setJobDescription(\"Step D: OCB 1/2 MB\")\n",
    "spark.conf.set(\"spark.sql.files.openCostInBytes\", 524288) # Reduce to 1/2 MB\n",
    "predict_num_partitions(trxFiles)                     \n",
    "spark.read.schema(trxSchema).parquet(trxPath).write.format(\"noop\").mode(\"overwrite\").save()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "571f2e64-d997-44a1-88cd-23d268ce022e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<table><tr><td>File Count:</td><td style=\"text-align:right; font-weight:bold\">1,467</td><td style=\"padding-left:1em\"></td></tr><tr><td>Actual Bytes:</td><td style=\"text-align:right; font-weight:bold\">654,715,376</td><td style=\"padding-left:1em\"></td></tr><tr><td>Padded Bytes:</td><td style=\"text-align:right; font-weight:bold\">846,998,000</td><td style=\"padding-left:1em\">Actual_Bytes + (File_Count * Open_Cost)</td></tr><tr><td>Average Size:</td><td style=\"text-align:right; font-weight:bold\">577,367</td><td style=\"padding-left:1em\"></td></tr><tr><td colspan=\"2\" style=\"border-top:1px solid black\">&nbsp;</td></tr><tr><td>Open Cost:</td><td style=\"text-align:right; font-weight:bold\">131,072</td><td style=\"padding-left:1em\">spark.sql.files.openCostInBytes</td></tr><tr><td>Bytes-Per-Core:</td><td style=\"text-align:right; font-weight:bold\">84,699,800</td><td style=\"padding-left:1em\"></td></tr><tr><td>Max Cost:</td><td style=\"text-align:right; font-weight:bold\">84,699,800</td><td style=\"padding-left:1em\">(max of Open_Cost & Bytes-Per-Core)</td></tr><tr><td colspan=\"2\" style=\"border-top:1px solid black\">&nbsp;</td></tr><tr><td>Max Partition Bytes:</td><td style=\"text-align:right; font-weight:bold\">134,217,728</td><td style=\"padding-left:1em\">spark.sql.files.maxPartitionBytes</td></tr><tr><td>Target Size:</td><td style=\"text-align:right; font-weight:bold\">84,699,800</td><td style=\"padding-left:1em\">(min of Max_Cost & Max_Partition_Bytes)</td></tr><tr><td colspan=\"2\" style=\"border-top:1px solid black\">&nbsp;</td></tr><tr><td>Number of Partitions:</td><td style=\"text-align:right; font-weight:bold\">10</td><td style=\"padding-left:1em\">(10.0 from Padded_Bytes / Target_Size)</td></tr></table>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sc.setJobDescription(\"Step E: OCB 1/8 MB\")\n",
    "spark.conf.set(\"spark.sql.files.openCostInBytes\", 131072) # Reduce to 1/8 MB\n",
    "predict_num_partitions(trxFiles)                     \n",
    "spark.read.schema(trxSchema).parquet(trxPath).write.format(\"noop\").mode(\"overwrite\").save()   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "713e7621-8fee-45cd-a365-c56395be4ed4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##  Results Summary\n",
    "\n",
    "| Step | `openCostInBytes` | Padded Size | Predicted Partitions | Spark Tasks | Runtime | Notes |\n",
    "|------|-------------------|-------------|------------------------|-------------|---------|-------|\n",
    "| C    | 4 MB              | 6.8 GB      | 51                     | 51          | 56 s    | Default setting |\n",
    "| D    | 0.5 MB            | 1.4 GB      | 11                     | 12          | 51 s    | Much less padding |\n",
    "| E    | 0.125 MB          | 847 MB      | 10                     | 11          | 39 s    | Padded ≈ actual |\n",
    "| F    | 0 MB              | 654 MB      | 5                      | 6           | 37 s    | Only real size used |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e85fde72-2314-4243-9f3f-99fb45c03030",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##  Observations\n",
    "\n",
    "- As `openCostInBytes ↓`, the **padded size ↓**, so Spark creates **fewer partitions**.\n",
    "- With fewer partitions, the **number of tasks ↓** in Stage 0.\n",
    "- **Job runtime improved** when reducing from 51 to ~10 tasks.\n",
    "- Below a certain point (around 10 tasks), **runtime gains flatten** — fewer tasks = less parallelism.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "964bd915-3786-4357-91d9-a52be63e2d46",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##  When to Adjust `openCostInBytes`\n",
    "\n",
    "| Scenario | Recommendation |\n",
    "|----------|----------------|\n",
    "| You have **many small files** | Keep a **higher** open cost (e.g. 4 MB) to avoid huge partitions full of tiny files |\n",
    "| Your files are **large** already | `openCostInBytes` has **little to no effect** |\n",
    "| You want **fewer partitions** and trust the file system | Consider reducing `openCostInBytes` |\n",
    "| You use **autotuning or dynamic partitioning** | Spark might override this value during optimization |\n",
    "\n",
    "---\n",
    "\n",
    "## Key Insight\n",
    "\n",
    "> `openCostInBytes` is a **soft penalty** added to each file to model I/O overhead.  \n",
    "> It's not about physical size, but about **how Spark plans the workload.**\n",
    "\n",
    "Reducing it gives more compact jobs, but **too few tasks** might underutilize your cluster.\n",
    "\n",
    "Notice that it is important to use a moderate value of `openCostInBytes` if I have just a few small files, as in this case with 1.2K files."
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "2. openCostInBytes",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}